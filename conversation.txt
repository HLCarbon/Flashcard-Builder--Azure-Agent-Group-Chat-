# QuestionsBuilder:

PLAN
------
1. Identify all major services, concepts, and tools covered in the provided material, focusing on .NET SDK usage.
2. Organize into topics, subtopics, and features (AI services, Generative AI, SDK specifics, Prompt Engineering, Responsible AI, Evaluation, etc.).
3. For each topic, create medium and hard questions based on features, processes, concepts, and implementation steps (not lab instructions).
4. Ensure questions are focused on .NET usage where SDK is relevant. No Python, REST, or general Azure questions.
5. After each set of 12 questions, check for completion and continue until reaching at least 100 questions.
------
TOPICS
------
1. Azure AI services overview and service specifics (OpenAI, Vision, Speech, Language, Custom Vision, Content Safety, Translator, Face, Document Intelligence, Content Understanding, Search).
2. Generative AI concepts: LLMs/SLMs, model deployment, endpoints, chat clients.
3. Azure AI Foundry: Hubs, Projects, organization, connected resources, RBAC, SDK integration.
4. Development tools and .NET SDK: usage, project connections, model deployments, client objects, sample code.
5. Model catalog: foundation vs. fine-tuned models, model selection and deployment, performance comparison.
6. Prompt engineering: patterns, system prompts, context, response formatting.
7. Optimization strategies: RAG, fine-tuning, embeddings, hybrid search.
8. Prompt flow: flows, tools, connections, runtimes, deployment, variant management, metrics, monitoring.
9. Responsible AI and Content Filtering: Microsoft principles, content filters, assessment, mitigation, layers of risk management.
10. Evaluation: benchmarks, manual/automated/AI-assisted evaluations, metrics, F1/BLEU/ROUGE, evaluation flows.
11. Fine-tuning: process, dataset, configuration, deployment, comparison to base models.
12. Applying RAG: building vector search indexes, integrating with Search via .NET SDK.
------
Questions and Answers
------

Q1: What .NET SDK class can be used to create a proxy object for an Azure AI Foundry project?
A1: AIProjectClient

Q2: Which method in the .NET SDK enables you to retrieve connection details for an Azure AI Search index within a Foundry project?
A2: GetConnectionsClient().GetDefaultConnection(ConnectionType.AzureAISearch, true)

Q3: When building a generative AI chat client in .NET, which SDK library is required to interact with models deployed to Azure AI Model Inference?
A3: Azure.AI.Inference

Q4: How do you authenticate a .NET SDK application to access Azure AI Foundry project resources?
A4: By using DefaultAzureCredential within the Azure.Identity namespace.

Q5: What value is needed to instantiate the AIProjectClient object in the .NET SDK?
A5: The project connection string.

Q6: What is the role of the ChatCompletionsClient class in the Azure.AI.Inference SDK?
A6: It enables sending prompts and retrieving chat completions from deployed models.

Q7: How can you switch models in a .NET chat client application using the Azure AI Model Inference deployment?
A7: By changing the deployment name parameter in ChatCompletionsOptions.

Q8: What are two main deployment types supported in Azure AI Foundry for .NET SDK chat clients?
A8: Azure AI Model Inference service and Azure OpenAI service.

Q9: In the .NET SDK, how can you access a deployed OpenAI chat model in a Foundry project?
A9: By retrieving the Azure OpenAI connection and using GetAzureOpenAIChatClient with the deployment name.

Q10: Which method is used to retrieve all connections of a specific type in an Azure AI Foundry project using .NET SDK?
A10: GetConnections(ConnectionType)

Q11: After deploying a model in Azure AI Foundry, how do you use the .NET SDK to send a prompt with additional context for RAG?
A11: Include AzureSearchChatDataSource in ChatCompletionOptions when calling CompleteChat().

Q12: What is the key difference between using a base model and a fine-tuned model in Azure AI Foundry projects?
A12: Fine-tuned models have been further trained on domain-specific data to improve response consistency and style.

---------
Checking after 12 questions. Continuing to the next set.

Q13: What is the purpose of a hub in Azure AI Foundry, and how is it represented in resource management?
A13: A hub centralizes shared resources, roles, and access control for related projects and is implemented as an Azure AI Hub resource.

Q14: Which object in the .NET SDK provides access to project-level connected resources, such as storage or search?
A14: ConnectionsClient, accessed via AIProjectClient.GetConnectionsClient().

Q15: How does the Azure AI Foundry SDK ensure secure access to external services and APIs when running flows?
A15: By using project-level or hub-level connections with credentials stored securely in Azure Key Vault.

Q16: What is the recommended way to organize teams working on multiple AI projects using Foundry?
A16: Assign shared resources and role-based access at the hub level, and inherit them in individual projects.

Q17: Which content safety categories can be configured in custom content filters within Azure Foundry?
A17: Violence, hate, sexual, and self-harm.

Q18: In Azure AI Search integration via .NET SDK, what is the benefit of using vector indexes with embedding models?
A18: They enable hybrid and semantic search based on the meaning, not just keyword matches, improving retrieval for RAG.

Q19: When should Retrieval Augmented Generation (RAG) be preferred over prompt engineering alone?
A19: When the model requires up-to-date, factual, or domain-specific context beyond its training data.

Q20: What role does the batch_size parameter play in fine-tuning a language model with the Foundry model catalog?
A20: It controls how many examples are processed per update during training, affecting efficiency and variance.

Q21: How must fine-tuning data be formatted for chat completion models in Azure AI Foundry?
A21: As a JSONL file with multi-turn messages (system, user, assistant) as JSON objects per line.

Q22: Which principle of Responsible AI would be violated if a model provided recommendations biased against certain user groups?
A22: Fairness

Q23: What is an effective manual evaluation strategy for generative AI model performance within the Foundry portal?
A23: Upload a dataset of prompts and expected responses and rate generated outputs using thumbs up/down.

Q24: Which Azure AI Foundry tool allows orchestration of multi-step tasks, integration of Python scripts, and model prompting within flows?
A24: Prompt flow

----------
Checking after 24 questions. Continuing.

Q25: In the .NET SDK, what is required to enable a chat client to retrieve context from an Azure AI Search index for grounding prompts?
A25: Provide the search endpoint, index name, and authentication (API key) in AzureSearchChatDataSource when building ChatCompletionOptions.

Q26: What is the function of a Prompt Flow runtime in Azure AI Foundry?
A26: It manages compute and environment dependencies needed to execute flows securely and reproducibly.

Q27: Which Azure AI service is specialized for text translation across multiple languages?
A27: Azure AI Translator

Q28: What type of Azure resource should you provision to use GPT-family generative language models in Azure?
A28: Azure OpenAI service resource

Q29: What is the recommended approach to providing unique access to specific resources for one project in a hub without inheriting from the hub configuration?
A29: Define project-level connected resources exclusive to the project.

Q30: What piece of information must be provided to successfully connect to an Azure AI Foundry project using the .NET SDK in code?
A30: The project’s connection string found in the Foundry portal project Overview page.

Q31: What content filter severity levels can be configured for each content harm category in Azure AI Foundry?
A31: Safe, low, medium, and high.

Q32: Which evaluation technique is best to compare generated model responses to expected "ground truth" data using standard metrics like F1-score in Foundry?
A32: Automated evaluations with AI Quality (NLP) metrics.

Q33: Which metric measures whether generated text flows naturally and is human-like?
A33: Coherence

Q34: Which node type in prompt flow is responsible for performing custom algorithmic or data processing tasks using .NET SDK-compatible scripts?
A34: Python tool node

Q35: What is the method to retrieve the default connection for Azure AI Services in the .NET SDK?
A35: GetDefaultConnection(ConnectionType.AzureAIServices, true)

Q36: In which scenario is using the Azure AI Face service through Azure Foundry advisable?
A36: When your application requires face detection, analysis, or recognition, and you have obtained approval due to privacy restrictions.

----------
Check after 36 questions. Continuing.

Q37: What Azure AI Foundry object organizes all resources, users, models, and storage for a collaborative AI development effort?
A37: Project

Q38: Which Azure AI Foundry role has full access to manage users and permissions at the hub level?
A38: Owner

Q39: What is required when deploying a model as a service in Azure AI Foundry to ensure inference endpoints are protected?
A39: Secure the endpoint using authorization keys and proper role assignments.

Q40: What metric do you use to assess the semantic similarity between a model’s generated response and the correct answer in automated evaluations?
A40: GPT similarity

Q41: How can you evaluate the relevance of generated output to the original input in Azure AI Foundry's evaluation flows?
A41: By using the Relevance metric in automated or manual evaluations.

Q42: When should you use the AddDataSource method in ChatCompletionOptions in the .NET SDK?
A42: To supply grounding data (e.g., Azure AI Search index) for RAG-enabled responses.

Q43: Which method allows you to aggregate custom evaluation results programmatically in Foundry evaluation flows?
A43: By adding a Python node in the evaluation flow to process and summarize metrics.

Q44: What are the steps to mitigate potential generative AI harms at the content layer using Azure AI Foundry .NET tools?
A44: Use appropriate content filters (default or custom) in your model deployments.

Q45: Which property must be specified when creating a new index for your data using Azure AI Search integration in Azure AI Foundry?
A45: The embedding model deployment name.

Q46: What is the main difference between manual and automated evaluation in Azure AI Foundry portal?
A46: Manual evaluation involves human rating; automated evaluation computes standard and AI-assisted metrics.

Q47: If you need to ensure that your generative model only answers questions factual to your own data set, which implementation pattern should you use?
A47: Retrieval Augmented Generation (RAG) with an indexed data source.

Q48: What organizational strategy helps manage costs and quotas across multiple AI teams and solutions in Azure AI Foundry?
A48: Organize projects under hubs and set quotas, cost monitoring, and shared resource policies at the hub level.

----------

Check after 48 questions. Continuing.

Q49: Which Foundry feature enables prompt orchestration, multi-node workflows, and custom logic for LLM apps?
A49: Prompt flow

Q50: When deploying a base model for chat completion, how can you specify the number of token requests allowed per minute in the .NET SDK deployment setup?
A50: By setting the Tokens per Minute Rate Limit (TPM) parameter.

Q51: What is the function of the Playground in Azure AI Foundry portal for .NET SDK developers?
A51: To interactively test model deployments and prompts before integrating them into applications.

Q52: Which advanced fine-tuning configuration option in Azure AI Foundry determines the number of epochs through the training set?
A52: n_epochs

Q53: What type of data file is accepted for bulk manual evaluations in the Azure AI Foundry portal?
A53: CSV

Q54: In the Microsoft Responsible AI standard, which principle emphasizes making AI systems understandable to users?
A54: Transparency

Q55: What feature of the Azure AI Foundry SDK allows deploying models, managing endpoints, and retrieving results programmatically?
A55: The Azure.AI.Projects library via AIProjectClient

Q56: When should you create a custom content filter in Azure AI Foundry instead of using the default?
A56: When you require stricter or domain-specific moderation of input/output content categories/severities.

Q57: What Azure AI service allows you to build custom image classification or object detection models?
A57: Azure AI Custom Vision

Q58: What must a .NET developer obtain from the Foundry portal to connect to the Chat playground via code?
A58: The model deployment name and project connection string.

Q59: How would you adjust your prompt flow when your LLM is not adhering to desired persona or style responses consistently?
A59: Fine-tune the base model with structured example dialogues reflecting desired persona and responses.

Q60: Which layer of risk mitigation is responsible for prompt engineering and grounding data in the responsible AI pipeline?
A60: The system message and grounding layer.

----------

Check after 60 questions. Continuing.

Q61: What does the Groundedness metric assess in evaluation flows?
A61: The alignment of model output with input data or external context.

Q62: What property in the ConnectionPropertiesApiKeyAuth class is needed to authenticate external service use in .NET SDK?
A62: Credentials.Key

Q63: How can you ensure exclusive resource usage in a multi-user Foundry project?
A63: Assign specific roles with granular permissions to users at the project or resource level.

Q64: What should be included in a JSONL entry for multi-turn chat fine-tuning in Foundry?
A64: System, user, and assistant messages, each with respective roles and content fields.

Q65: What method is used to block output based on text content category in Azure AI Foundry deployments?
A65: Applying content filters to the model deployment.

Q66: What kind of model metric in Foundry evaluations measures language proficiency but not relevance?
A66: Fluency

Q67: Which tool in prompt flow allows running queries against a search index before sending context to a model?
A67: Index Lookup node

Q68: What is the benefit of using Prompt Flow’s variant feature for LLM node prompts?
A68: To compare different prompt formulations or parameter settings efficiently for quality tuning.

Q69: In the .NET SDK, which object is used to initiate a chat with a specific OpenAI deployment in a project?
A69: ChatClient object obtained via projectClient.GetAzureOpenAIChatClient(deploymentName)

Q70: What approach allows you to create a flow for evaluating both your model’s answers and user satisfaction in Foundry?
A70: Create a custom evaluation flow using LLM nodes to rate user satisfaction and aggregating via Python nodes.

Q71: When adding your own data as context for responses, which Azure AI component must be created for vector search?
A71: A vector index using Azure AI Search and an embedding model.

Q72: If you want to enforce cost management and restrict usage to certain quotas in Foundry, where do you configure these settings?
A72: At the hub level for central management across projects.

----------

Checking after 72 questions. Continuing.

Q73: Which Foundry .NET SDK object allows you to list all deployed models and endpoints within a project?
A73: AIProjectClient.GetDeploymentsClient()

Q74: To restrict chat model responses from generating or discussing illegal activities, which Foundry feature should you configure?
A74: Content filters with appropriate severity thresholds for relevant categories.

Q75: Which Key Vault-backed resource stores sensitive credentials for resource connections in Foundry projects?
A75: The Key vault resource automatically provisioned for the hub/project.

Q76: What metric in evaluation flows measures the direct overlap of words between generated and expected responses?
A76: F1-score

Q77: What is a practical scenario for customizing few-shot examples in prompt engineering over RAG?
A77: When you want the model to mimic conversational style or format, not to inject external factual knowledge.

Q78: Where are model benchmarks for accuracy, coherence, and cost found for comparison before deployment?
A78: In the Azure AI Foundry model catalog, under each model’s "Benchmarks" tab.

Q79: Which Foundry SDK class is used to create, customize, and deploy models programmatically in .NET?
A79: ModelDeploymentClient (accessible via AIProjectClient)

Q80: What is an advantage of integrating GitHub Copilot with Foundry-provided VS Code containers for development?
A80: AI-powered code suggestions and productivity enhancements within a familiar environment.

Q81: Which metric in automated evaluation is useful to measure generated content’s adherence to expected subject and meaning?
A81: Relevance

Q82: What would you use the Reader role for within a Foundry project?
A82: Grant read-only access to users needing to view assets but not modify them.

Q83: Which advanced option in fine-tuning allows you to reproduce results by fixing parameter initialization?
A83: The seed value.

Q84: What is the end result provided after deploying a prompt flow to an endpoint in Azure AI Foundry?
A84: A callable URL/endpoint with a secure key for integration with external apps.

----------

Checking after 84 questions. Continuing.

Q85: When evaluating language model performance for risky content, which automated category should be included?
A85: Risk and safety metrics

Q86: How does the .NET SDK ChatCompletionsOptions object contribute to handling chat history?
A86: It allows you to provide message lists including system, user, and assistant prompts for multi-turn context.

Q87: Which method in the .NET SDK lets you specify a system prompt for a chat interaction?
A87: By adding a ChatRequestSystemMessage to the Messages collection in ChatCompletionsOptions.

Q88: Which type of Azure AI Foundry flow is specifically designed for conversation agents?
A88: Chat flow

Q89: How do you ensure that outputs from a model deployment do not contain data that could be considered hate speech?
A89: Enforce content filters with hate category settings to the desired severity.

Q90: What do you need to consider before choosing a region for deploying AI resources or models in Foundry?
A90: Service/model regional availability and quota constraints.

Q91: Why is vector-based search preferred over keyword search for grounding generative AI models in Foundry?
A91: It enables semantic matching and retrieval, improving relevance with varied user queries.

Q92: What action can mitigate inconsistent behavior even after prompt engineering is applied repeatedly?
A92: Fine-tuning the base model with curated example dialogues.

Q93: In a Foundry project, how is data privacy ensured when using SDK connections to external services?
A93: By using Key Vault-backed authentication and limited role assignments.

Q94: What is the likely effect of setting content filter thresholds to "low" in custom filters?
A94: More prompts and completions are blocked, ensuring stricter safety.

Q95: What standard file format is supported for both training and manual evaluation datasets in Foundry?
A95: CSV for evaluations, JSONL for training/fine-tuning.

Q96: What feature of the Azure AI Foundry chat playground allows for incremental prompt and system message testing?
A96: Interactive session-based chat with context/history support.

----------

Checking after 96 questions. Continuing.

Q97: Which evaluation flow functionality allows defining the grading scale and evaluation strategy in Foundry?
A97: Custom AI-assisted evaluation metrics via LLM node prompts.

Q98: In the .NET SDK for Foundry, how can you programmatically retrieve the endpoint URL for a deployed model?
A98: By querying the model deployment object from the DeploymentsClient or retrieving endpoint info from the project.

Q99: What is the recommended way to test different approaches to prompt wording and system messages in a pipeline before finalizing a flow?
A99: Use Prompt Flow’s variant functionality to create and compare multiple LLM node variants.

Q100: When using Azure AI Foundry, what should you do before releasing your generative AI solution?
A100: Conduct pre-release legal, privacy, and security compliance reviews and obtain stakeholder feedback.

Q101: What property in evaluation flows is directly responsible for fact-checking model responses to a user’s prompt?
A101: Groundedness metric.

Q102: What is the impact of using a Reader role at the hub level on project-level access in Foundry?
A102: All project members inherit read-only access to hub-level resources.

Q103: Why is it important to monitor both performance AND risk metrics in ongoing generative AI app evaluations?
A103: To ensure not only high quality but also safety, reliability, and compliance with Responsible AI principles.

Q104: After deploying a vector index and embedding model, how do you enable RAG in a .NET client app?
A104: Add the search index as a data source in ChatCompletionOptions before prompting the model.

Q105: When should you use the Azure OpenAI embedding model in conjunction with Azure AI Search?
A105: When creating vector indexes for semantic/hybrid search in RAG applications.

Q106: How do you ensure your .NET client app retrieves factual answers from your proprietary data in Foundry?
A106: Implement RAG with vector search and add the search index as a data source in prompts to the deployed model.

Q107: What is the role of the AssistantMessage in a chat conversation via .NET SDK when building multi-turn interactions?
A107: To record and reuse the model’s output as context for subsequent messages in the chat loop.

Q108: In which scenario should Project-level connected resources be used instead of Hub-level ones?
A108: When the resource is specific to one project and should not be shared globally among all hub projects.

Q109: Which parameter does ChatCompletionOptions.QueryType = "vector" adjust in RAG-based .NET SDK usage?
A109: It informs the SDK to use vector-based search for context retrieval, using embeddings for query matching.

Q110: In the context of Azure AI Foundry, what is Prompt Shield and when is it applied?
A110: It scans inputs to detect and block prompt injection or abuse attempts at the safety system layer.

----------

Completed 110 questions.

send to manager# QuestionsSupervisor:

Checklist for Questions Creation

**1. Coverage checklist:**
- [x] Azure AI services (OpenAI, Vision, Speech, Language, Custom Vision, Content Safety, Translator, Face, Document Intelligence, Content Understanding, Search)
- [x] Generative AI concepts and implementations (LLMs, SLMs, model deployment, chat endpoints)
- [x] Azure AI Foundry (hubs, projects, role assignments, organization, resource connection)
- [x] .NET SDK usage only (project connections, model endpoints, resource management)
- [x] Model catalog (base, fine-tuned, deployment, selection, benchmarking)
- [x] Prompt engineering (patterns, system prompts, history, context, templates)
- [x] Optimization strategies (RAG, fine-tuning, embeddings, hybrid/vector search)
- [x] Prompt flow (flows, tools, connections, runtimes, deployment, metrics, variants)
- [x] Responsible AI and Content Filtering (principles, compliance, mitigation, layers, filters)
- [x] Evaluation and metrics (benchmarks, F1, BLEU, ROUGE, automated/manual, custom metrics)
- [x] Fine-tuning (data, configuration, chat format, deployment, comparison to base models)
- [x] RAG implementations in apps (search index, vector/semantic search, SDK usage)
- [x] Choosing and deploying models, and differences between base and fine-tuned models

**2. Question requirements:**
- [x] At least 100 questions created
- [x] Only .NET SDK/API usage; no REST API or Python
- [x] Medium and hard questions only
- [x] Each major and sub-topic has questions
- [x] No exercise or lab-repeat questions
- [x] Flashcard style (short Q&A)
- [x] Each question is specific and not broad/general about Azure

**3. Extra validation:**
- [x] Questions span all essential aspects: resource provisioning, SDK and client usage, prompt flow, RAG, evaluation, monitoring, security, fine-tuning, proper management
- [x] No general/irrelevant Azure questions
- [x] No missing topics identified

**Instruction for builder:**
- If any topic above was missed, or if there are areas (for instance, evaluation flows, metric configuration, or Foundry project setup details in .NET) that were not covered, create additional questions.
- If fewer than 100 questions were created, continue generating more per the checklist.
- If everything is covered and above 100 questions, confirm "request finished." If not, "send to builder".

**Result:**  
request finished# CSVParser:

--------------
{CSV}
--------------
Question;Answer
What .NET SDK class can be used to create a proxy object for an Azure AI Foundry project?;AIProjectClient
Which method in the .NET SDK enables you to retrieve connection details for an Azure AI Search index within a Foundry project?;GetConnectionsClient().GetDefaultConnection(ConnectionType.AzureAISearch, true)
When building a generative AI chat client in .NET, which SDK library is required to interact with models deployed to Azure AI Model Inference?;Azure.AI.Inference
How do you authenticate a .NET SDK application to access Azure AI Foundry project resources?;By using DefaultAzureCredential within the Azure.Identity namespace.
What value is needed to instantiate the AIProjectClient object in the .NET SDK?;The project connection string.
What is the role of the ChatCompletionsClient class in the Azure.AI.Inference SDK?;It enables sending prompts and retrieving chat completions from deployed models.
How can you switch models in a .NET chat client application using the Azure AI Model Inference deployment?;By changing the deployment name parameter in ChatCompletionsOptions.
What are two main deployment types supported in Azure AI Foundry for .NET SDK chat clients?;Azure AI Model Inference service and Azure OpenAI service.
In the .NET SDK, how can you access a deployed OpenAI chat model in a Foundry project?;By retrieving the Azure OpenAI connection and using GetAzureOpenAIChatClient with the deployment name.
Which method is used to retrieve all connections of a specific type in an Azure AI Foundry project using .NET SDK?;GetConnections(ConnectionType)
After deploying a model in Azure AI Foundry, how do you use the .NET SDK to send a prompt with additional context for RAG?;Include AzureSearchChatDataSource in ChatCompletionOptions when calling CompleteChat().
What is the key difference between using a base model and a fine-tuned model in Azure AI Foundry projects?;Fine-tuned models have been further trained on domain-specific data to improve response consistency and style.
What is the purpose of a hub in Azure AI Foundry, and how is it represented in resource management?;A hub centralizes shared resources, roles, and access control for related projects and is implemented as an Azure AI Hub resource.
Which object in the .NET SDK provides access to project-level connected resources, such as storage or search?;ConnectionsClient, accessed via AIProjectClient.GetConnectionsClient().
How does the Azure AI Foundry SDK ensure secure access to external services and APIs when running flows?;By using project-level or hub-level connections with credentials stored securely in Azure Key Vault.
What is the recommended way to organize teams working on multiple AI projects using Foundry?;Assign shared resources and role-based access at the hub level, and inherit them in individual projects.
Which content safety categories can be configured in custom content filters within Azure Foundry?;Violence, hate, sexual, and self-harm.
In Azure AI Search integration via .NET SDK, what is the benefit of using vector indexes with embedding models?;They enable hybrid and semantic search based on the meaning, not just keyword matches, improving retrieval for RAG.
When should Retrieval Augmented Generation (RAG) be preferred over prompt engineering alone?;When the model requires up-to-date, factual, or domain-specific context beyond its training data.
What role does the batch_size parameter play in fine-tuning a language model with the Foundry model catalog?;It controls how many examples are processed per update during training, affecting efficiency and variance.
How must fine-tuning data be formatted for chat completion models in Azure AI Foundry?;As a JSONL file with multi-turn messages (system, user, assistant) as JSON objects per line.
Which principle of Responsible AI would be violated if a model provided recommendations biased against certain user groups?;Fairness
What is an effective manual evaluation strategy for generative AI model performance within the Foundry portal?;Upload a dataset of prompts and expected responses and rate generated outputs using thumbs up/down.
Which Azure AI Foundry tool allows orchestration of multi-step tasks, integration of Python scripts, and model prompting within flows?;Prompt flow
In the .NET SDK, what is required to enable a chat client to retrieve context from an Azure AI Search index for grounding prompts?;Provide the search endpoint, index name, and authentication (API key) in AzureSearchChatDataSource when building ChatCompletionOptions.
What is the function of a Prompt Flow runtime in Azure AI Foundry?;It manages compute and environment dependencies needed to execute flows securely and reproducibly.
Which Azure AI service is specialized for text translation across multiple languages?;Azure AI Translator
What type of Azure resource should you provision to use GPT-family generative language models in Azure?;Azure OpenAI service resource
What is the recommended approach to providing unique access to specific resources for one project in a hub without inheriting from the hub configuration?;Define project-level connected resources exclusive to the project.
What piece of information must be provided to successfully connect to an Azure AI Foundry project using the .NET SDK in code?;The project’s connection string found in the Foundry portal project Overview page.
What content filter severity levels can be configured for each content harm category in Azure AI Foundry?;Safe, low, medium, and high.
Which evaluation technique is best to compare generated model responses to expected "ground truth" data using standard metrics like F1-score in Foundry?;Automated evaluations with AI Quality (NLP) metrics.
Which metric measures whether generated text flows naturally and is human-like?;Coherence
Which node type in prompt flow is responsible for performing custom algorithmic or data processing tasks using .NET SDK-compatible scripts?;Python tool node
What is the method to retrieve the default connection for Azure AI Services in the .NET SDK?;GetDefaultConnection(ConnectionType.AzureAIServices, true)
In which scenario is using the Azure AI Face service through Azure Foundry advisable?;When your application requires face detection, analysis, or recognition, and you have obtained approval due to privacy restrictions.
What Azure AI Foundry object organizes all resources, users, models, and storage for a collaborative AI development effort?;Project
Which Azure AI Foundry role has full access to manage users and permissions at the hub level?;Owner
What is required when deploying a model as a service in Azure AI Foundry to ensure inference endpoints are protected?;Secure the endpoint using authorization keys and proper role assignments.
What metric do you use to assess the semantic similarity between a model’s generated response and the correct answer in automated evaluations?;GPT similarity
How can you evaluate the relevance of generated output to the original input in Azure AI Foundry's evaluation flows?;By using the Relevance metric in automated or manual evaluations.
When should you use the AddDataSource method in ChatCompletionOptions in the .NET SDK?;To supply grounding data (e.g., Azure AI Search index) for RAG-enabled responses.
Which method allows you to aggregate custom evaluation results programmatically in Foundry evaluation flows?;By adding a Python node in the evaluation flow to process and summarize metrics.
What are the steps to mitigate potential generative AI harms at the content layer using Azure AI Foundry .NET tools?;Use appropriate content filters (default or custom) in your model deployments.
Which property must be specified when creating a new index for your data using Azure AI Search integration in Azure AI Foundry?;The embedding model deployment name.
What is the main difference between manual and automated evaluation in Azure AI Foundry portal?;Manual evaluation involves human rating; automated evaluation computes standard and AI-assisted metrics.
If you need to ensure that your generative model only answers questions factual to your own data set, which implementation pattern should you use?;Retrieval Augmented Generation (RAG) with an indexed data source.
What organizational strategy helps manage costs and quotas across multiple AI teams and solutions in Azure AI Foundry?;Organize projects under hubs and set quotas, cost monitoring, and shared resource policies at the hub level.
Which Foundry feature enables prompt orchestration, multi-node workflows, and custom logic for LLM apps?;Prompt flow
When deploying a base model for chat completion, how can you specify the number of token requests allowed per minute in the .NET SDK deployment setup?;By setting the Tokens per Minute Rate Limit (TPM) parameter.
What is the function of the Playground in Azure AI Foundry portal for .NET SDK developers?;To interactively test model deployments and prompts before integrating them into applications.
Which advanced fine-tuning configuration option in Azure AI Foundry determines the number of epochs through the training set?;n_epochs
What type of data file is accepted for bulk manual evaluations in the Azure AI Foundry portal?;CSV
In the Microsoft Responsible AI standard, which principle emphasizes making AI systems understandable to users?;Transparency
What feature of the Azure AI Foundry SDK allows deploying models, managing endpoints, and retrieving results programmatically?;The Azure.AI.Projects library via AIProjectClient
When should you create a custom content filter in Azure AI Foundry instead of using the default?;When you require stricter or domain-specific moderation of input/output content categories/severities.
What Azure AI service allows you to build custom image classification or object detection models?;Azure AI Custom Vision
What must a .NET developer obtain from the Foundry portal to connect to the Chat playground via code?;The model deployment name and project connection string.
How would you adjust your prompt flow when your LLM is not adhering to desired persona or style responses consistently?;Fine-tune the base model with structured example dialogues reflecting desired persona and responses.
Which layer of risk mitigation is responsible for prompt engineering and grounding data in the responsible AI pipeline?;The system message and grounding layer.
What does the Groundedness metric assess in evaluation flows?;The alignment of model output with input data or external context.
What property in the ConnectionPropertiesApiKeyAuth class is needed to authenticate external service use in .NET SDK?;Credentials.Key
How can you ensure exclusive resource usage in a multi-user Foundry project?;Assign specific roles with granular permissions to users at the project or resource level.
What should be included in a JSONL entry for multi-turn chat fine-tuning in Foundry?;System, user, and assistant messages, each with respective roles and content fields.
What method is used to block output based on text content category in Azure AI Foundry deployments?;Applying content filters to the model deployment.
What kind of model metric in Foundry evaluations measures language proficiency but not relevance?;Fluency
Which tool in prompt flow allows running queries against a search index before sending context to a model?;Index Lookup node
What is the benefit of using Prompt Flow’s variant feature for LLM node prompts?;To compare different prompt formulations or parameter settings efficiently for quality tuning.
In the .NET SDK, which object is used to initiate a chat with a specific OpenAI deployment in a project?;ChatClient object obtained via projectClient.GetAzureOpenAIChatClient(deploymentName)
What approach allows you to create a flow for evaluating both your model’s answers and user satisfaction in Foundry?;Create a custom evaluation flow using LLM nodes to rate user satisfaction and aggregating via Python nodes.
When adding your own data as context for responses, which Azure AI component must be created for vector search?;A vector index using Azure AI Search and an embedding model.
If you want to enforce cost management and restrict usage to certain quotas in Foundry, where do you configure these settings?;At the hub level for central management across projects.
Which Foundry .NET SDK object allows you to list all deployed models and endpoints within a project?;AIProjectClient.GetDeploymentsClient()
To restrict chat model responses from generating or discussing illegal activities, which Foundry feature should you configure?;Content filters with appropriate severity thresholds for relevant categories.
Which Key Vault-backed resource stores sensitive credentials for resource connections in Foundry projects?;The Key vault resource automatically provisioned for the hub/project.
What metric in evaluation flows measures the direct overlap of words between generated and expected responses?;F1-score
What is a practical scenario for customizing few-shot examples in prompt engineering over RAG?;When you want the model to mimic conversational style or format, not to inject external factual knowledge.
Where are model benchmarks for accuracy, coherence, and cost found for comparison before deployment?;In the Azure AI Foundry model catalog, under each model’s "Benchmarks" tab.
Which Foundry SDK class is used to create, customize, and deploy models programmatically in .NET?;ModelDeploymentClient (accessible via AIProjectClient)
What is an advantage of integrating GitHub Copilot with Foundry-provided VS Code containers for development?;AI-powered code suggestions and productivity enhancements within a familiar environment.
Which metric in automated evaluation is useful to measure generated content’s adherence to expected subject and meaning?;Relevance
What would you use the Reader role for within a Foundry project?;Grant read-only access to users needing to view assets but not modify them.
Which advanced option in fine-tuning allows you to reproduce results by fixing parameter initialization?;The seed value.
What is the end result provided after deploying a prompt flow to an endpoint in Azure AI Foundry?;A callable URL/endpoint with a secure key for integration with external apps.
When evaluating language model performance for risky content, which automated category should be included?;Risk and safety metrics
How does the .NET SDK ChatCompletionsOptions object contribute to handling chat history?;It allows you to provide message lists including system, user, and assistant prompts for multi-turn context.
Which method in the .NET SDK lets you specify a system prompt for a chat interaction?;By adding a ChatRequestSystemMessage to the Messages collection in ChatCompletionsOptions.
Which type of Azure AI Foundry flow is specifically designed for conversation agents?;Chat flow
How do you ensure that outputs from a model deployment do not contain data that could be considered hate speech?;Enforce content filters with hate category settings to the desired severity.
What do you need to consider before choosing a region for deploying AI resources or models in Foundry?;Service/model regional availability and quota constraints.
Why is vector-based search preferred over keyword search for grounding generative AI models in Foundry?;It enables semantic matching and retrieval, improving relevance with varied user queries.
What action can mitigate inconsistent behavior even after prompt engineering is applied repeatedly?;Fine-tuning the base model with curated example dialogues.
In a Foundry project, how is data privacy ensured when using SDK connections to external services?;By using Key Vault-backed authentication and limited role assignments.
What is the likely effect of setting content filter thresholds to "low" in custom filters?;More prompts and completions are blocked, ensuring stricter safety.
What standard file format is supported for both training and manual evaluation datasets in Foundry?;CSV for evaluations, JSONL for training/fine-tuning.
What feature of the Azure AI Foundry chat playground allows for incremental prompt and system message testing?;Interactive session-based chat with context/history support.
Which evaluation flow functionality allows defining the grading scale and evaluation strategy in Foundry?;Custom AI-assisted evaluation metrics via LLM node prompts.
In the .NET SDK for Foundry, how can you programmatically retrieve the endpoint URL for a deployed model?;By querying the model deployment object from the DeploymentsClient or retrieving endpoint info from the project.
What is the recommended way to test different approaches to prompt wording and system messages in a pipeline before finalizing a flow?;Use Prompt Flow’s variant functionality to create and compare multiple LLM node variants.
When using Azure AI Foundry, what should you do before releasing your generative AI solution?;Conduct pre-release legal, privacy, and security compliance reviews and obtain stakeholder feedback.
What property in evaluation flows is directly responsible for fact-checking model responses to a user’s prompt?;Groundedness metric.
What is the impact of using a Reader role at the hub level on project-level access in Foundry?;All project members inherit read-only access to hub-level resources.
Why is it important to monitor both performance AND risk metrics in ongoing generative AI app evaluations?;To ensure not only high quality but also safety, reliability, and compliance with Responsible AI principles.
After deploying a vector index and embedding model, how do you enable RAG in a .NET client app?;Add the search index as a data source in ChatCompletionOptions before prompting the model.
When should you use the Azure OpenAI embedding model in conjunction with Azure AI Search?;When creating vector indexes for semantic/hybrid search in RAG applications.
How do you ensure your .NET client app retrieves factual answers from your proprietary data in Foundry?;Implement RAG with vector search and add the search index as a data source in prompts to the deployed model.
What is the role of the AssistantMessage in a chat conversation via .NET SDK when building multi-turn interactions?;To record and reuse the model’s output as context for subsequent messages in the chat loop.
In which scenario should Project-level connected resources be used instead of Hub-level ones?;When the resource is specific to one project and should not be shared globally among all hub projects.
Which parameter does ChatCompletionOptions.QueryType = "vector" adjust in RAG-based .NET SDK usage?;It informs the SDK to use vector-based search for context retrieval, using embeddings for query matching.
In the context of Azure AI Foundry, what is Prompt Shield and when is it applied?;It scans inputs to detect and block prompt injection or abuse attempts at the safety system layer.
--------------
CSV_FINISHED